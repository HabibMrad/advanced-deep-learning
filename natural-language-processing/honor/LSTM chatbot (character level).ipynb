{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/scratch/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import datasets\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powered by https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    https://stackoverflow.com/questions/47923370/keras-bidirectional-lstm-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:03<00:00, 23531.72it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.readCornellData(\"data/cornell\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ./download_opensubs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = datasets.readOpensubsData(\"data/opensubs/\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#start_symbol = '^'\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'\n",
    "unk_symbol = \"|\"\n",
    "start_symbol=\"^\"\n",
    "all_sym = [start_symbol,end_symbol, padding_symbol, unk_symbol] + \\\n",
    "    list(Counter(chain.from_iterable(chain.from_iterable(data))).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {symbol:i for i, symbol in enumerate(all_sym)}\n",
    "id2char = {i:symbol for symbol, i in char2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2i(sentence, word2id=char2id, padded_len=22):\n",
    "    \"\"\" Converts a sequence of words to a padded sequence of their ids.\n",
    "\n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    # Account for start and end\n",
    "    sentence=\"^\"+sentence\n",
    "    sentence = [i for i in sentence]\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    sent_ids = [word2id[i] if i in word2id.keys() else word2id[\"|\"]\n",
    "                for i in sentence]\n",
    "    \n",
    "    sent_len = len(sent_ids[:padded_len-1])+1\n",
    "    sent_ids = sent_ids[:padded_len-1]+[word2id[\"$\"]] + \\\n",
    "        [word2id[\"#\"]]*(padded_len-len(sent_ids)-1)\n",
    "\n",
    "   # return (sent_ids, sent_len-2)\n",
    "    return sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([w2i(i[0]) for i in data])\n",
    "y = np.array([w2i(i[1]) for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_pad=np.ones((y.shape[0],1),dtype=np.int)*char2id[\"#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shift=np.concatenate([y[:,1:],more_pad],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehot encode\n",
    "X_ohot = np.zeros(shape=(X.shape[0],X.shape[1],len(id2char)),dtype=\"float32\")\n",
    "y_ohot = np.zeros(shape=(X.shape[0],X.shape[1],len(id2char)),dtype=\"float32\")\n",
    "y_shift_ohot = np.zeros(shape=(X.shape[0],X.shape[1],len(id2char)),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[0]):\n",
    "    for k,character in enumerate(X[i,:]):\n",
    "        X_ohot[i,k,character]=1\n",
    "    for k,character in enumerate(y[i,:]):\n",
    "        y_ohot[i,k,character]=1\n",
    "    for k,character in enumerate(y_shift[i,:]):\n",
    "        y_shift_ohot[i,k,character]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Input, Dense, Embedding,Bidirectional,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "latent_dim = 150\n",
    "num_tokens = len(id2char)\n",
    "batch_size=64\n",
    "epochs=12\n",
    "max_len=X.shape[1]\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(None,num_tokens))\n",
    "\n",
    "#pre_encoder=LSTM(latent_dim, return_state=True,dropout=0.5,return_sequences=True)(encoder_input)\n",
    "encoder=LSTM(latent_dim, return_state=True,return_sequences=True)\n",
    "\n",
    "\n",
    "encoder_outputs, state_h, state_c=encoder(encoder_input)\n",
    "#encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_input)\n",
    "encoder_states=[state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input=Input(shape=(None,num_tokens))\n",
    "#pre_decoder=LSTM(latent_dim, return_state=True,dropout=0.5,return_sequences=True)\n",
    "decoder_lstm=LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_,_=decoder_lstm(\n",
    "   decoder_input, initial_state=encoder_states)\n",
    "decoder_dense=Dense(num_tokens, activation=\"softmax\")\n",
    "decoder_output=decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model=Model([encoder_input, decoder_input], decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam =Adam(lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, None, 41)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, None, 41)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, None, 150),  115200      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, None, 150),  115200      input_16[0][0]                   \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 41)     6191        lstm_14[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 236,591\n",
      "Trainable params: 236,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=adam,metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19833 samples, validate on 4959 samples\n",
      "Epoch 1/10\n",
      "19833/19833 [==============================] - 17s 873us/step - loss: 0.8278 - acc: 0.7406 - val_loss: 0.8708 - val_acc: 0.7321\n",
      "Epoch 2/10\n",
      "19833/19833 [==============================] - 17s 833us/step - loss: 0.8145 - acc: 0.7451 - val_loss: 0.8655 - val_acc: 0.7335\n",
      "Epoch 3/10\n",
      "19833/19833 [==============================] - 16s 824us/step - loss: 0.8051 - acc: 0.7482 - val_loss: 0.8614 - val_acc: 0.7349\n",
      "Epoch 4/10\n",
      "19833/19833 [==============================] - 17s 859us/step - loss: 0.7964 - acc: 0.7501 - val_loss: 0.8579 - val_acc: 0.7368\n",
      "Epoch 5/10\n",
      "19833/19833 [==============================] - 17s 882us/step - loss: 0.7878 - acc: 0.7528 - val_loss: 0.8557 - val_acc: 0.7362\n",
      "Epoch 6/10\n",
      "19833/19833 [==============================] - 19s 968us/step - loss: 0.7800 - acc: 0.7551 - val_loss: 0.8525 - val_acc: 0.7378\n",
      "Epoch 7/10\n",
      "19833/19833 [==============================] - 20s 991us/step - loss: 0.7716 - acc: 0.7579 - val_loss: 0.8505 - val_acc: 0.7390\n",
      "Epoch 8/10\n",
      "19833/19833 [==============================] - 18s 906us/step - loss: 0.7640 - acc: 0.7604 - val_loss: 0.8489 - val_acc: 0.7396\n",
      "Epoch 9/10\n",
      "19833/19833 [==============================] - 18s 896us/step - loss: 0.7564 - acc: 0.7627 - val_loss: 0.8476 - val_acc: 0.7401\n",
      "Epoch 10/10\n",
      "19833/19833 [==============================] - 18s 885us/step - loss: 0.7492 - acc: 0.7649 - val_loss: 0.8475 - val_acc: 0.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8f9e48cf8>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[X_ohot, y_ohot], y=y_shift_ohot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_output, state_hd, state_cd = decoder_lstm(\n",
    "    decoder_input, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hd, state_cd]\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "decoder_model = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoder_output] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, char2id, max_len):\n",
    "    input_seq = ohot_input(input_seq, char2id, max_len)\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(char2id)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, char2id[\"^\"]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "       \n",
    "        sampled_char = id2char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '$' or\n",
    "                len(decoded_sentence) > max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    decoded_sentence = (decoded_sentence.replace(\"$\", \"\")\n",
    "                        .replace(\"i m\", \"i'm\")\n",
    "                        .replace(\"what s\", \"what's\")\n",
    "                        .replace(\"don t\", \"don't\")\n",
    "                        .replace(\"that s\", \"that's\")\n",
    "                        .replace(\"you re\", \"you're\")\n",
    "                        .replace(\"he s\", \"he's\")\n",
    "                       )\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohot_input(x,char2id,max_len):\n",
    "    nums=w2i(x,word2id=char2id,padded_len=max_len)\n",
    "    out = np.zeros((1,max_len,len(char2id)))\n",
    "    \n",
    "    for k,character in enumerate(nums):\n",
    "        out[0,k,character]=1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i dont know'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(\"how are you\",encoder_model,decoder_model,char2id,max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
