{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/scratch/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import datasets\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is just taking week4's notebook and plugging it into the chatbot, with some tweaks to make it work with sentences (small changes in embedding size and max_iter).\n",
    "\n",
    "I tried different things (Amounts of dropout, learning rates and learning rate schedules, GRUs and LSTM cells and I didn't manage to get results better than what you see here, which admittedly are not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:03<00:00, 22632.32it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.readCornellData(\"data/cornell\", max_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ./download_opensubs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = datasets.readOpensubsData(\"data/opensubs/\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "#start_symbol = '^'\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'\n",
    "start_symbol=\"^\"\n",
    "all_sym = [start_symbol,end_symbol, padding_symbol] + \\\n",
    "    list(Counter(chain.from_iterable(chain.from_iterable(data))).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {symbol:i for i, symbol in enumerate('^$#abcdefghijklmnopqrstuvwxyz 0123456789+-')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    \n",
    "    sent_ids = [word2id[i] for i in sentence]\n",
    "    sent_len = len(sent_ids[:padded_len-1])+1\n",
    "    sent_ids = sent_ids[:padded_len-1]+[word2id[\"$\"]]+[word2id[\"#\"]]*(padded_len-len(sent_ids)-1)\n",
    "    \n",
    "    return (sent_ids, sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(\n",
    "            shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(\n",
    "            shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(\n",
    "            shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "        self.ground_truth_lengths = tf.placeholder(\n",
    "            shape=(None, ), dtype=tf.int32, name='ground_truth_length')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(\n",
    "            tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "\n",
    "    def __create_embeddings(self, vocab_size, embeddings_size):\n",
    "        \"\"\"Specifies embeddings layer and embeds an input batch.\"\"\"\n",
    "\n",
    "        random_initializer = tf.random_uniform(\n",
    "            (vocab_size, embeddings_size), -1.0, 1.0)\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=random_initializer, dtype=tf.float32)\n",
    "\n",
    "        # Perform embeddings lookup for self.input_batch.\n",
    "        self.input_batch_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, self.input_batch)\n",
    "\n",
    "    def __build_encoder(self, hidden_size):\n",
    "        \"\"\"Specifies encoder architecture and computes its output.\"\"\"\n",
    "\n",
    "        # Create GRUCell with dropout.\n",
    "        encoder_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(hidden_size),\n",
    "                                                     input_keep_prob=self.dropout_ph)\n",
    "\n",
    "        # Create RNN with the predefined cell.\n",
    "        _, self.final_encoder_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, self.input_batch_embedded, sequence_length=self.input_batch_lengths, dtype=tf.float32)\n",
    "\n",
    "    def __build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "        \"\"\"Specifies decoder architecture and computes the output.\n",
    "\n",
    "            Uses different helpers:\n",
    "              - for train: feeding ground truth\n",
    "              - for inference: feeding generated output\n",
    "\n",
    "            As a result, self.train_outputs and self.infer_outputs are created. \n",
    "            Each of them contains two fields:\n",
    "              rnn_output (predicted logits)\n",
    "              sample_id (predictions).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Use start symbols as the decoder inputs at the first time step.\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "        ground_truth_as_input = tf.concat(\n",
    "            [tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "\n",
    "        # Use the embedding layer defined before to lookup embedings for ground_truth_as_input.\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, ground_truth_as_input)\n",
    "\n",
    "        # Create TrainingHelper for the train stage.\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "\n",
    "        # Create GreedyEmbeddingHelper for the inference stage.\n",
    "        # You should provide the embedding layer, start_tokens and index of the end symbol.\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=self.embeddings, start_tokens=word2id[\"^\"]*tf.ones([batch_size], dtype=tf.int32),\n",
    "                                                                end_token=word2id[\"$\"])\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                decoder_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(hidden_size, reuse=reuse),\n",
    "                                                             input_keep_prob=self.dropout_ph)\n",
    "\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                    decoder_cell, vocab_size, reuse=reuse)\n",
    "\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    decoder_cell, helper=helper, initial_state=self.final_encoder_state)\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=max_iter,\n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "\n",
    "    def __compute_loss(self):\n",
    "        \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "        \n",
    "        weights = tf.cast(tf.sequence_mask(\n",
    "            self.ground_truth_lengths), dtype=tf.float32)\n",
    "  \n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(self.train_outputs.rnn_output,\n",
    "                                                     self.ground_truth, weights=weights)\n",
    "\n",
    "    def __perform_optimization(self):\n",
    "        \"\"\"Specifies train_op that optimizes self.loss.\"\"\"\n",
    "\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(self.loss, optimizer=\"Adam\", learning_rate=self.learning_rate_ph,\n",
    "                                                        global_step=tf.train.get_global_step(), clip_gradients=1.0)\n",
    "    def __init__(self, vocab_size, embeddings_size, hidden_size, \n",
    "                   max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "\n",
    "        self.__declare_placeholders()\n",
    "        self.__create_embeddings(vocab_size, embeddings_size)\n",
    "        self.__build_encoder(hidden_size)\n",
    "        self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "\n",
    "        # Compute loss and back-propagate.\n",
    "        self.__compute_loss()\n",
    "        self.__perform_optimization()\n",
    "\n",
    "        # Get predictions for evaluation.\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "                self.input_batch: X,\n",
    "                self.input_batch_lengths: X_seq_len,\n",
    "                self.ground_truth: Y,\n",
    "                self.ground_truth_lengths: Y_seq_len,\n",
    "                self.learning_rate_ph: learning_rate,\n",
    "                self.dropout_ph: dropout_keep_probability\n",
    "            }\n",
    "        pred, loss, _ = session.run([\n",
    "                self.train_predictions,\n",
    "                self.loss,\n",
    "                self.train_op], feed_dict=feed_dict)\n",
    "        return pred, loss\n",
    "    \n",
    "    def predict_for_batch(self, session, X, X_seq_len):\n",
    "        feed_dict = {self.input_batch: X, self.input_batch_lengths: X_seq_len}\n",
    "        pred = session.run([\n",
    "                self.infer_predictions\n",
    "            ], feed_dict=feed_dict)[0]\n",
    "        return pred\n",
    "\n",
    "    def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "        feed_dict = {\n",
    "                self.input_batch: X,\n",
    "                self.input_batch_lengths: X_seq_len,\n",
    "                self.ground_truth: Y,\n",
    "                self.ground_truth_lengths: Y_seq_len}\n",
    "        pred, loss = session.run([\n",
    "                self.infer_predictions,\n",
    "                self.loss,\n",
    "            ], feed_dict=feed_dict)\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "n_epochs = 4\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.7\n",
    "max_len = 30\n",
    "\n",
    "n_step = int(len(data) / batch_size)\n",
    "\n",
    "model = Seq2SeqModel(vocab_size=len(word2id), embeddings_size=40, max_iter=max_len, hidden_size=256,\n",
    "                     start_symbol_id=word2id[\"^\"], end_symbol_id=word2id[\"$\"], padding_symbol_id=word2id[\"#\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 1\n",
      "Epoch: [1/4], step: [1/310], loss: 1.386444\n",
      "Epoch: [1/4], step: [11/310], loss: 1.406645\n",
      "Epoch: [1/4], step: [21/310], loss: 1.408774\n",
      "Epoch: [1/4], step: [31/310], loss: 1.388344\n",
      "Epoch: [1/4], step: [41/310], loss: 1.392088\n",
      "Epoch: [1/4], step: [51/310], loss: 1.438425\n",
      "Epoch: [1/4], step: [61/310], loss: 1.445774\n",
      "Epoch: [1/4], step: [71/310], loss: 1.430758\n",
      "Epoch: [1/4], step: [81/310], loss: 1.367387\n",
      "Epoch: [1/4], step: [91/310], loss: 1.417184\n",
      "Epoch: [1/4], step: [101/310], loss: 1.445110\n",
      "Epoch: [1/4], step: [111/310], loss: 1.449245\n",
      "Epoch: [1/4], step: [121/310], loss: 1.409753\n",
      "Epoch: [1/4], step: [131/310], loss: 1.461484\n",
      "Epoch: [1/4], step: [141/310], loss: 1.411704\n",
      "Epoch: [1/4], step: [151/310], loss: 1.360555\n",
      "Epoch: [1/4], step: [161/310], loss: 1.405701\n",
      "Epoch: [1/4], step: [171/310], loss: 1.422820\n",
      "Epoch: [1/4], step: [181/310], loss: 1.446037\n",
      "Epoch: [1/4], step: [191/310], loss: 1.424580\n",
      "Epoch: [1/4], step: [201/310], loss: 1.446039\n",
      "Epoch: [1/4], step: [211/310], loss: 1.390620\n",
      "Epoch: [1/4], step: [221/310], loss: 1.445808\n",
      "Epoch: [1/4], step: [231/310], loss: 1.446217\n",
      "Epoch: [1/4], step: [241/310], loss: 1.405913\n",
      "Test: epoch 1 loss: 1.4759214\n",
      "Train: epoch 2\n",
      "Epoch: [2/4], step: [1/310], loss: 1.444994\n",
      "Epoch: [2/4], step: [11/310], loss: 1.429778\n",
      "Epoch: [2/4], step: [21/310], loss: 1.405355\n",
      "Epoch: [2/4], step: [31/310], loss: 1.427693\n",
      "Epoch: [2/4], step: [41/310], loss: 1.435700\n",
      "Epoch: [2/4], step: [51/310], loss: 1.436268\n",
      "Epoch: [2/4], step: [61/310], loss: 1.429879\n",
      "Epoch: [2/4], step: [71/310], loss: 1.398598\n",
      "Epoch: [2/4], step: [81/310], loss: 1.417994\n",
      "Epoch: [2/4], step: [91/310], loss: 1.440255\n",
      "Epoch: [2/4], step: [101/310], loss: 1.459306\n",
      "Epoch: [2/4], step: [111/310], loss: 1.458706\n",
      "Epoch: [2/4], step: [121/310], loss: 1.398771\n",
      "Epoch: [2/4], step: [131/310], loss: 1.517695\n",
      "Epoch: [2/4], step: [141/310], loss: 1.431483\n",
      "Epoch: [2/4], step: [151/310], loss: 1.381155\n",
      "Epoch: [2/4], step: [161/310], loss: 1.386074\n",
      "Epoch: [2/4], step: [171/310], loss: 1.413970\n",
      "Epoch: [2/4], step: [181/310], loss: 1.366591\n",
      "Epoch: [2/4], step: [191/310], loss: 1.435140\n",
      "Epoch: [2/4], step: [201/310], loss: 1.444394\n",
      "Epoch: [2/4], step: [211/310], loss: 1.434765\n",
      "Epoch: [2/4], step: [221/310], loss: 1.430157\n",
      "Epoch: [2/4], step: [231/310], loss: 1.465399\n",
      "Epoch: [2/4], step: [241/310], loss: 1.372761\n",
      "Test: epoch 2 loss: 1.410552\n",
      "Train: epoch 3\n",
      "Epoch: [3/4], step: [1/310], loss: 1.417446\n",
      "Epoch: [3/4], step: [11/310], loss: 1.389804\n",
      "Epoch: [3/4], step: [21/310], loss: 1.454495\n",
      "Epoch: [3/4], step: [31/310], loss: 1.376858\n",
      "Epoch: [3/4], step: [41/310], loss: 1.368886\n",
      "Epoch: [3/4], step: [51/310], loss: 1.361470\n",
      "Epoch: [3/4], step: [61/310], loss: 1.438445\n",
      "Epoch: [3/4], step: [71/310], loss: 1.396832\n",
      "Epoch: [3/4], step: [81/310], loss: 1.389067\n",
      "Epoch: [3/4], step: [91/310], loss: 1.424761\n",
      "Epoch: [3/4], step: [101/310], loss: 1.382054\n",
      "Epoch: [3/4], step: [111/310], loss: 1.400098\n",
      "Epoch: [3/4], step: [121/310], loss: 1.411997\n",
      "Epoch: [3/4], step: [131/310], loss: 1.411221\n",
      "Epoch: [3/4], step: [141/310], loss: 1.398780\n",
      "Epoch: [3/4], step: [151/310], loss: 1.389140\n",
      "Epoch: [3/4], step: [161/310], loss: 1.425641\n",
      "Epoch: [3/4], step: [171/310], loss: 1.448110\n",
      "Epoch: [3/4], step: [181/310], loss: 1.403284\n",
      "Epoch: [3/4], step: [191/310], loss: 1.423880\n",
      "Epoch: [3/4], step: [201/310], loss: 1.396524\n",
      "Epoch: [3/4], step: [211/310], loss: 1.416366\n",
      "Epoch: [3/4], step: [221/310], loss: 1.409067\n",
      "Epoch: [3/4], step: [231/310], loss: 1.391897\n",
      "Epoch: [3/4], step: [241/310], loss: 1.433929\n",
      "Test: epoch 3 loss: 1.465074\n",
      "Train: epoch 4\n",
      "Epoch: [4/4], step: [1/310], loss: 1.394737\n",
      "Epoch: [4/4], step: [11/310], loss: 1.424868\n",
      "Epoch: [4/4], step: [21/310], loss: 1.434517\n",
      "Epoch: [4/4], step: [31/310], loss: 1.387390\n",
      "Epoch: [4/4], step: [41/310], loss: 1.417741\n",
      "Epoch: [4/4], step: [51/310], loss: 1.396476\n",
      "Epoch: [4/4], step: [61/310], loss: 1.425628\n",
      "Epoch: [4/4], step: [71/310], loss: 1.412702\n",
      "Epoch: [4/4], step: [81/310], loss: 1.454777\n",
      "Epoch: [4/4], step: [91/310], loss: 1.449535\n",
      "Epoch: [4/4], step: [101/310], loss: 1.484820\n",
      "Epoch: [4/4], step: [111/310], loss: 1.431916\n",
      "Epoch: [4/4], step: [121/310], loss: 1.425552\n",
      "Epoch: [4/4], step: [131/310], loss: 1.396882\n",
      "Epoch: [4/4], step: [141/310], loss: 1.353047\n",
      "Epoch: [4/4], step: [151/310], loss: 1.393173\n",
      "Epoch: [4/4], step: [161/310], loss: 1.427266\n",
      "Epoch: [4/4], step: [171/310], loss: 1.394160\n",
      "Epoch: [4/4], step: [181/310], loss: 1.416172\n",
      "Epoch: [4/4], step: [191/310], loss: 1.430687\n",
      "Epoch: [4/4], step: [201/310], loss: 1.490613\n",
      "Epoch: [4/4], step: [211/310], loss: 1.448806\n",
      "Epoch: [4/4], step: [221/310], loss: 1.400750\n",
      "Epoch: [4/4], step: [231/310], loss: 1.355022\n",
      "Epoch: [4/4], step: [241/310], loss: 1.442783\n",
      "Test: epoch 4 loss: 1.4485799\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        X, X_len = batch_to_ids(X_batch, word2id, max_len=max_len)\n",
    "        Y, Y_len = batch_to_ids(Y_batch, word2id, max_len=max_len)\n",
    "        predictions, loss = model.predict_for_batch_with_loss(\n",
    "            session=session, X=X, X_seq_len=X_len, Y=Y,\n",
    "            Y_seq_len=Y_len)\n",
    "        model.train_on_batch(session, X, X_len, Y, Y_len,\n",
    "                             learning_rate, dropout_keep_probability)\n",
    "        if n_iter % 10 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" %\n",
    "                  (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "    \n",
    "    \n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "\n",
    "    X, X_len = batch_to_ids(X_sent, word2id, max_len=max_len)\n",
    "\n",
    "    Y, Y_len = batch_to_ids(Y_sent, word2id, max_len=max_len)\n",
    "    predictions, loss = model.predict_for_batch_with_loss(\n",
    "        session=session, X=X, X_seq_len=X_len, Y=Y,\n",
    "        Y_seq_len=Y_len)\n",
    "\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_for_batch(session, X, X_len)\n",
    "model_predictions = np.array(\n",
    "            [\"\".join(ids_to_sentence(i, id2word)) for i in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes i dont know$^^^^^^^^^^^^^^', 'yes i dont know$^^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'he was a second of the back$^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'he was a second of the back$^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'you dont know what i said it$^',\n",
       "       'he was a second of the back$^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'yes i dont know$^^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'he was a second of the back$^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'you dont know what i said it $', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'yes$^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'the money is the back to the $',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'he was a second of the back$^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'yes i dont know$^^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know what i said it was', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'yes i dont know$^^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'he was a second of the back$^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'you dont know what i said it $', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'yes$^^^^^^^^^^^^^^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'you dont know what i said it $',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'you dont know what i said it $',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'the car what do you mean$^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'you dont know what i said it $', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'you dont know what i said it $',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'you dont know what i said it$^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'the money is the back to the $',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know what i said it was',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'he was a second of the back$^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'yes$^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'you dont know what i said it $',\n",
       "       'i dont know i dont know$^^^^^^', 'he was a second of the back$^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'the best be a second of the $^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'he was a second of the back$^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'the car what do you mean$^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'you dont know what i said it $', 'he was a second of the back$^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'yes i dont know$^^^^^^^^^^^^^^', 'you dont know what i said it $',\n",
       "       'i dont know what i said it was', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'yes i dont know$^^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^',\n",
       "       'i dont know what i said it was', 'i dont know i dont know$^^^^^^',\n",
       "       'what do you mean$^^^^^^^^^^^^^', 'what do you mean$^^^^^^^^^^^^^',\n",
       "       'i dont know i dont know$^^^^^^', 'i dont know i dont know$^^^^^^'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(question,word2id,max_len,model,id2word):\n",
    "\n",
    "    ids, ids_len = sentence_to_ids(question,word2id,padded_len=max_len)\n",
    "    ids=np.array(ids).reshape(1,len(ids))\n",
    "\n",
    "    ids_len=np.array(ids_len).reshape(1)\n",
    "    predictions = model.predict_for_batch(session, ids, ids_len)\n",
    "    return \"\".join(ids_to_sentence(predictions[0], id2word)).replace(\"$\",\"\").capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"hi\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dont know i dont know'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"how are you\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you mean'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"blerg\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not so good responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dont know i dont know'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"tell me about you\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dont know what i said it was'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"why\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was a second of the back'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"hello\",word2id,15,model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/chatbot_model'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,\"../model/chatbot_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit goes in the existing chatbot code, in a tfchatbot.py file to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import *\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'\n",
    "start_symbol=\"^\"\n",
    "word2id = {symbol:i for i, symbol in enumerate('^$#abcdefghijklmnopqrstuvwxyz 0123456789+-')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}\n",
    "max_len = 30\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('model/chatbot_model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "sess.run(tf.local_variables_initializer())\n",
    "def reply(question, word2id, max_len, id2word, session):\n",
    "    input_batch = tf.get_default_graph().get_tensor_by_name(\"input_batch:0\")\n",
    "    input_batch_len = tf.get_default_graph().get_tensor_by_name(\"input_batch_lengths:0\")\n",
    "    infer_predictions = tf.get_default_graph().get_tensor_by_name(\n",
    "        \"decode_1/decoder/transpose_1:0\")\n",
    "\n",
    "    question = text_prepare(question)\n",
    "    ids, ids_len = sentence_to_ids(question, word2id, padded_len=max_len)\n",
    "    ids = np.array(ids).reshape(1, len(ids))\n",
    "\n",
    "    ids_len = np.array(ids_len).reshape(1)\n",
    "    predictions = session.run([\n",
    "        infer_predictions\n",
    "    ], feed_dict={input_batch: ids, input_batch_len: ids_len})[0]\n",
    "    return \"\".join(ids_to_sentence(predictions[0], id2word)).replace(\"$\", \"\").capitalize()\n",
    "reply(\"hi\",word2id,max_len,id2word,sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
