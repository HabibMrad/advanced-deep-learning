{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Kung-Fu with advantage actor-critic\n",
    "\n",
    "In this notebook you'll build a deep reinforcement learning agent for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/) and train it with advantage actor-critic.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a look at the game itself:\n",
    "* Image resized to 42x42 and grayscale to run faster\n",
    "* Rewards divided by 100 'cuz they are all divisible by 100\n",
    "* Agent sees last 4 frames of game to account for object velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape: (42, 42, 4)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 5:],\n",
    "                          dim_order = 'tensorflow',\n",
    "                          color=False, n_frames=4,\n",
    "                          reward_scale = 0.01)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8a9c1bc240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8a9c520eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation (4-frame buffer)')\n",
    "plt.imshow(s.transpose([0,2,1]).reshape([42,-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an agent\n",
    "\n",
    "We now have to build an agent for actor-critic training - a convolutional neural network that converts states into action probabilities $\\pi$ and state values $V$.\n",
    "\n",
    "Your assignment here is to build and apply a neural network - with any framework you want. \n",
    "\n",
    "For starters, we want you to implement this architecture:\n",
    "![https://s17.postimg.org/orswlfzcv/nnet_arch.png](https://s17.postimg.org/orswlfzcv/nnet_arch.png)\n",
    "\n",
    "After your agent gets mean reward above 50, we encourage you to experiment with model architecture to score even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/scratch/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            # prepare a graph for agent step\n",
    "            \n",
    "            \n",
    "            inputs = Input(shape=state_shape)\n",
    "            x = Conv2D(filters=32,kernel_size=3,strides=2,activation=\"selu\")(inputs)\n",
    "            x = Conv2D(filters=32,kernel_size=3,strides=2,activation=\"selu\")(x)\n",
    "            x = Conv2D(filters=32,kernel_size=3,strides=2,activation=\"selu\")(x)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(256,activation=\"selu\")(x)\n",
    "            x = Dense(256,activation=\"selu\")(x)\n",
    "            \n",
    "            out_action_logits = Dense(n_actions)(x)\n",
    "            out_state_value   = Dense(1)(x)\n",
    "            \n",
    "            self.network = Model(inputs=inputs,outputs=[out_action_logits,out_state_value])\n",
    "            \n",
    "            # Prepare neural network architecture\n",
    "            # Your code here: prepare any necessary layers, variables, etc.\n",
    "            self.state_t = tf.placeholder(\n",
    "                'float32', [None, ] + list(state_shape))\n",
    "            \n",
    "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
    "\n",
    "    def symbolic_step(self, state_t):\n",
    "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
    "\n",
    "        # Apply neural network\n",
    "        # Your code here: apply agent's neural network to get policy logits and state values.\n",
    "\n",
    "        logits,state_value = self.network(state_t)\n",
    "        state_value = state_value[:, 0]\n",
    "        \n",
    "\n",
    "        assert tf.is_numeric_tensor(state_value) and state_value.shape.ndims == 1, \\\n",
    "            \"please return 1D tf tensor of state values [you got %s]\" % repr(\n",
    "                state_value)\n",
    "        assert tf.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
    "            \"please return 2d tf tensor of logits [you got %s]\" % repr(logits)\n",
    "        # hint: if you triggered state_values assert with your shape being [None, 1],\n",
    "        # just select [:, 0]-th element of state values as new state values\n",
    "\n",
    "        return (logits, state_value)\n",
    "\n",
    "    def step(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        policy = np.exp(logits) / np.sum(np.exp(logits),\n",
    "                                         axis=-1, keepdims=True)\n",
    "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"agent\", obs_shape, n_actions)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " [[-0.96184087  0.19464749  0.19212249 -0.5017273  -0.13308981 -1.2565144\n",
      "  -0.71595633  0.23195441 -0.6100732   0.49543518 -1.4108405  -0.4552421\n",
      "  -0.7153689  -0.40718126]]\n",
      "state values:\n",
      " [-0.02439186]\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "logits, value = agent.step(state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.sample_actions(agent.step([state]))[0]\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "# rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "# env_monitor.close()\n",
    "# print (rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #show video\n",
    "# from IPython.display import HTML\n",
    "# import os\n",
    "\n",
    "# video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"640\" height=\"480\" controls>\n",
    "#   <source src=\"{}\" type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "![img](https://s7.postimg.org/4y36s2b2z/env_pool.png)\n",
    "\n",
    "To make actor-critic training more stable, we shall play several games in parallel. This means ya'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for ya - just make sure you read it before you use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "    def __init__(self, n_envs = 10):\n",
    "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
    "        return np.array([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Send a vector[batch_size] of actions into respective environments\n",
    "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
    "        \"\"\"\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
    "        \n",
    "        # reset environments automatically\n",
    "        for i in range(len(self.envs)):\n",
    "            if done[i]:\n",
    "                new_obs[i] = self.envs[i].reset()\n",
    "        \n",
    "        return new_obs, rewards, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "State shape: (10, 42, 42, 4)\n",
      "Actions: [ 0 13  1]\n",
      "Rewards: [0. 0. 0.]\n",
      "Done: [False False False]\n"
     ]
    }
   ],
   "source": [
    "env_batch = EnvBatch(10)\n",
    "\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
    "\n",
    "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "print(\"State shape:\", batch_states.shape)\n",
    "print(\"Actions:\", batch_actions[:3])\n",
    "print(\"Rewards:\", batch_rewards[:3])\n",
    "print(\"Done:\", batch_done[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic\n",
    "\n",
    "Here we define a loss functions and learning algorithms as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These placeholders mean exactly the same as in \"Let's try it out\" section above\n",
    "states_ph = tf.placeholder('float32', [None,] + list(obs_shape))    \n",
    "next_states_ph = tf.placeholder('float32', [None,] + list(obs_shape))\n",
    "actions_ph = tf.placeholder('int32', (None,))\n",
    "rewards_ph = tf.placeholder('float32', (None,))\n",
    "is_done_ph = tf.placeholder('float32', (None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
    "logits, state_values = agent.symbolic_step(states_ph)\n",
    "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
    "next_state_values = next_state_values * (1 - is_done_ph)\n",
    "\n",
    "# probabilities and log-probabilities for all actions\n",
    "probs = tf.nn.softmax(logits)            # [n_envs, n_actions]\n",
    "logprobs = tf.nn.log_softmax(logits)     # [n_envs, n_actions]\n",
    "\n",
    "# log-probabilities only for agent's chosen actions\n",
    "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compute advantage using rewards_ph, state_values and next_state_values\n",
    "gamma = 0.99\n",
    "lam = 0.7\n",
    "advantage = rewards_ph+gamma*next_state_values-state_values\n",
    "\n",
    "assert advantage.shape.ndims == 1, \"please compute advantage for each sample, vector of shape [n_envs,]\"\n",
    "\n",
    "# compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
    "entropy = -tf.reduce_sum(probs*logprobs,axis=-1)\n",
    "\n",
    "assert entropy.shape.ndims == 1, \"please compute pointwise entropy vector of shape [n_envs,] \"\n",
    "\n",
    "\n",
    "\n",
    "actor_loss =  - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage)) - 0.001 * tf.reduce_mean(entropy)\n",
    "\n",
    "# compute target state values using temporal difference formula. Use rewards_ph and next_step_values\n",
    "target_state_values = rewards_ph+gamma*next_state_values\n",
    "\n",
    "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2 )\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just might be fine!\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks to catch some errors. Specific to KungFuMaster in assignment's default setup.\n",
    "l_act, l_crit, adv, ent = sess.run([actor_loss, critic_loss, advantage, entropy], feed_dict = {\n",
    "        states_ph: batch_states,\n",
    "        actions_ph: batch_actions,\n",
    "        next_states_ph: batch_states,\n",
    "        rewards_ph: batch_rewards,\n",
    "        is_done_ph: batch_done,\n",
    "    })\n",
    "\n",
    "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
    "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
    "if ent.mean() < np.log(n_actions) / 2: print(\"Entropy is too low for untrained agent\")\n",
    "print(\"You just might be fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
    "![img](http://images6.fanpop.com/image/photos/38900000/Daniel-san-training-the-karate-kid-38947361-499-288.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "env_batch = EnvBatch(10)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "rewards_history = []\n",
    "entropy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jose/scratch/venv/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/jose/scratch/venv/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-742343c2c6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mrewards_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrewards_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your agent has earned the yellow belt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-8d2585e2cf70>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, env, n_games)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/advanced_deep_learning/reinforcement-learning/week5/atari_util.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/venv/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/venv/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/venv/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "ewma = lambda x,span: Series(x).ewm(span).mean()\n",
    "for i in trange(100000): \n",
    "    \n",
    "    batch_actions = agent.sample_actions(agent.step(batch_states))\n",
    "    batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "    \n",
    "    feed_dict = {\n",
    "        states_ph: batch_states,\n",
    "        actions_ph: batch_actions,\n",
    "        next_states_ph: batch_next_states,\n",
    "        rewards_ph: batch_rewards,\n",
    "        is_done_ph: batch_done,\n",
    "    }\n",
    "    batch_states = batch_next_states\n",
    "    \n",
    "    _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
    "    #Clip gradients\n",
    "    \n",
    "    \n",
    "    entropy_history.append(np.mean(ent_t))\n",
    "\n",
    "    if i % 500 == 0: \n",
    "        if i % 2500 == 0:\n",
    "            rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
    "            if rewards_history[-1] >= 50:\n",
    "                print(\"Your agent has earned the yellow belt\")\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[8,4])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(ewma(np.array(rewards_history),span=10), marker='.', label='rewards ewma@10')\n",
    "        plt.title(\"Session rewards\"); plt.grid(); plt.legend()\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(entropy_history, label='entropy')\n",
    "        plt.plot(ewma(np.array(entropy_history),span=1000), label='entropy ewma@1000')\n",
    "        plt.title(\"Policy entropy\"); plt.grid(); plt.legend()        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 23498/100000 [14:40<47:47, 26.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean reward: 452.0\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=1,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward:\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.2.11236.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't see videos, just navigate to ./kungfu_videos and download .mp4 files from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_kungfu\n",
    "env = make_env()\n",
    "submit_kungfu(env,agent, evaluate, .,.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "Well, 5k reward is [just the beginning](https://www.buzzfeed.com/mattjayyoung/what-the-color-of-your-karate-belt-actually-means-lg3g). Can you get past 200? With recurrent neural network memory, chances are you can even beat 400!\n",
    "\n",
    "* Try n-step advantage and \"lambda\"-advantage (aka GAE) - see [this article](https://arxiv.org/abs/1506.02438)\n",
    " * This change should improve early convergence a lot\n",
    "* Try recurrent neural network \n",
    " * RNN memory will slow things down initially, but in will reach better final reward at this game\n",
    "* Implement asynchronuous version\n",
    " * Remember [A3C](https://arxiv.org/abs/1602.01783)? The first \"A\" stands for asynchronuous. It means there are several parallel actor-learners out there.\n",
    " * You can write custom code for synchronization, but we recommend using [redis](https://redis.io/)\n",
    "   * You can store full parameter set in redis, along with any other metadate\n",
    "   * Here's a _quick_ way to (de)serialize parameters for redis\n",
    "   ```\n",
    "   import joblib\n",
    "   from six import BytesIO\n",
    "```\n",
    "```\n",
    "   def dumps(data):\n",
    "        \"converts whatever to string\"\n",
    "        s = BytesIO()\n",
    "        joblib.dump(data,s)\n",
    "        return s.getvalue()\n",
    "``` \n",
    "```\n",
    "    def loads(string):\n",
    "        \"converts string to whatever was dumps'ed in it\"\n",
    "        return joblib.load(BytesIO(string))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
